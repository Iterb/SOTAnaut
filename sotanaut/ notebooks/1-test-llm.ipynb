{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/sheep-duck-llama-2-13B-GPTQ\"\n",
    "model_basename = \"model\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-32g-actorder_True\"\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    model_basename=model_basename,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    device=\"cuda:0\",\n",
    "    use_triton=False,\n",
    "    quantize_config=None,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "system_message = \"Be truthfull\"\n",
    "prompt_template = f\"\"\"### System:\n",
    "{system_message}\n",
    "\n",
    "### User:\n",
    "{prompt}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(\n",
    "    inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512\n",
    ")\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=2048,\n",
    "    temperature=0,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    "    generation_config=generation_config,\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
